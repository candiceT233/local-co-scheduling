{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import traceback\n",
    "\n",
    "from itertools import islice\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import math\n",
    "\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "# pd.set_option('display.max_columns', 4)\n",
    "# pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO : contact map first\n",
    "1. splits logs into 3 parts, sim_write , agg_read , agg_write\n",
    "2. then in each part, splits into vol and vfd logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def check_empty(df,p=True):\n",
    "    nan_df = df[df.isna().any(axis=1)].copy()\n",
    "    if not nan_df.empty:\n",
    "        if p:\n",
    "            print(f\"NaN rows: {nan_df}\") # check any nan\n",
    "        return list(nan_df.index)\n",
    "    \n",
    "    null_df = df[df.isnull().any(axis=1)].copy()\n",
    "    if not null_df.empty:\n",
    "        if p:\n",
    "            print(f\"NULL rows: {null_df}\") # check any null\n",
    "        return list(null_df.index)\n",
    "    \n",
    "def rec_to_df(records):\n",
    "    df = pd.DataFrame.from_dict(records,orient='index')\n",
    "    df.replace('/mnt/ssd/mtang11/','',regex=True, inplace=True)\n",
    "    df.replace('molecular_dynamics_runs/stage0000/','',regex=True, inplace=True)\n",
    "    \n",
    "    # df['hash_id']= df['hash_id'].astype(str)\n",
    "    # df['hash_id'] = map(lambda x: x.encode('base64','strict'), df['hash_id'])\n",
    "    if 'logical_addr' in df.columns:\n",
    "        df['logical_addr'] = df['logical_addr'].fillna(-1.0) #.astype(int)\n",
    "    #df['logical_addr'] = df['logical_addr'].astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def df_to_csv(df,file_name,suffix=''):\n",
    "    # df.hash_id=df.hash_id.astype('category').cat.codes\n",
    "    # out_csv=file_name.replace('prov-vfd-','')\n",
    "    out_csv=file_name.replace('.log',f'{suffix}.csv')\n",
    "    df.to_csv(out_csv,index=False)\n",
    "\n",
    "def df_to_parquet(df,file_name,suffix=''):\n",
    "    out_parquet=file_name.replace('.log',f'{suffix}.parquet')\n",
    "    try:\n",
    "        df.to_parquet(out_parquet, engine='pyarrow') # compression='gzip'\n",
    "        # pyarrow has error with streamlit, downgrade to streamlit==0.84.2\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "\n",
    "def print_list_diff(list1, list2):\n",
    "    if list1 == list2:\n",
    "        print(\"same lists\")\n",
    "    else:\n",
    "        print(\"different lists\\nindex:\\t[list1]\\t[list2]\")\n",
    "        for index, (first, second) in enumerate(zip(list1, list2)):\n",
    "            if first != second:\n",
    "                # print(index, first, second)\n",
    "                print(f\"{index}:\\t{first}]\\t[{second}]\")\n",
    "\n",
    "\n",
    "def split_vfd_vol_rec(fname,mode='r'):\n",
    "    vol_r_ops = ['H5VLdataset_read', 'H5VLblob_get', 'H5FD__hermes_read']\n",
    "    vol_w_ops = ['H5VLdataset_write', 'H5VLblob_put', 'H5FD__hermes_write']\n",
    "    other_ops = ['_get','_create', '_close', '_open',]\n",
    "\n",
    "    vol_rec = {}\n",
    "    vfd_rec = {}\n",
    "    vol_idx = 0\n",
    "    vfd_idx = 0\n",
    "    with open(fname, mode) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                rec = ast.literal_eval(line)\n",
    "                \n",
    "                if any(op in rec['func_name'] for op in vol_r_ops):\n",
    "                    if 'hermes' in rec['func_name']:\n",
    "                        vfd_rec[vfd_idx] = rec\n",
    "                        vfd_rec[vfd_idx]['operation'] = 'write'\n",
    "                        vfd_idx+=1\n",
    "                    else:\n",
    "                        vol_rec[vol_idx] = rec\n",
    "                        vol_rec[vol_idx]['operation'] = 'read'\n",
    "                        vol_idx+=1\n",
    "                elif any(op in rec['func_name'] for op in vol_w_ops):\n",
    "                    if 'hermes' in rec['func_name']:\n",
    "                        vfd_rec[vfd_idx] = rec\n",
    "                        vfd_rec[vfd_idx]['operation'] = 'write'\n",
    "                        vfd_idx+=1\n",
    "                    else:\n",
    "                        vol_rec[vol_idx] = rec\n",
    "                        vol_rec[vol_idx]['operation'] = 'write'\n",
    "                        vol_idx+=1          \n",
    "                # not record other_ops\n",
    "                elif any(op in rec['func_name'] for op in other_ops):\n",
    "                    _suffix = rec['func_name'].split('_')[-1]\n",
    "                    if 'hermes' in rec['func_name']:\n",
    "                        vfd_rec[vfd_idx] = rec\n",
    "                        vfd_rec[vfd_idx]['operation'] = _suffix\n",
    "                        vfd_idx+=1\n",
    "                    else:\n",
    "                        vol_rec[vol_idx] = rec\n",
    "                        vol_rec[vol_idx]['operation'] = _suffix\n",
    "                        vol_idx+=1\n",
    "                    \n",
    "            except:\n",
    "                print(\"Erro line:\")\n",
    "                print(line)\n",
    "                # break\n",
    "    return vol_rec, vfd_rec\n",
    "\n",
    "def add_all_op_type(dfvol, dfvfd, vol_map={}, vfd_map={}):\n",
    "    if bool(vol_map) == False:\n",
    "        vol_map={'read':'data', 'write': 'data'}\n",
    "        vfd_map={'H5FD_MEM_DRAW':'data', 'H5FD_MEM_LHEAP': 'lheap'}\n",
    "\n",
    "    dfvol['op_type'] = dfvol['operation'].map(vol_map)\n",
    "    dfvol['op_type'] = dfvol['op_type'].fillna('meta')\n",
    "    \n",
    "    dfvfd['mem_type'] = dfvfd['mem_type'].fillna('H5FD_MEM_NTYPES')\n",
    "    dfvfd['op_type'] = dfvfd['mem_type'].map(vfd_map)\n",
    "    dfvfd['op_type'] = dfvfd['op_type'].fillna('meta')\n",
    "\n",
    "def add_vol_op_type(dfvol, vol_map={}):\n",
    "    if bool(vol_map) == False:\n",
    "        vol_map={'read':'data', 'write': 'data'}\n",
    "\n",
    "    dfvol['op_type'] = dfvol['operation'].map(vol_map)\n",
    "    dfvol['op_type'] = dfvol['op_type'].fillna('meta')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in simulation data\n",
    "fsim='../save_outputs/vol-vfd/prov-vfd-sim.log'\n",
    "mode='r'\n",
    "\n",
    "vol_sim_rec, vfd_sim_rec = split_vfd_vol_rec(fsim)\n",
    "\n",
    "vol_sdf = rec_to_df(vol_sim_rec)\n",
    "vfd_sdf = rec_to_df(vfd_sim_rec)\n",
    "\n",
    "# add category\n",
    "vol_sdf['cat'] = 'sim-write'\n",
    "vfd_sdf['cat'] = 'sim-write'\n",
    "\n",
    "def vol_data_label(df):\n",
    "    data_idx_list = []\n",
    "    dsets = list(set(df['file_no']))\n",
    "    for index, row in df.iterrows():\n",
    "        _pat = row['op_type']\n",
    "        _dset = row['dset_name']\n",
    "        _str = f\"vol-{_pat}-{_dset}\"\n",
    "        if _pat =='data':\n",
    "            _str = f\"vol-{_pat}-{_dset}-{row['io_access_idx']}\"\n",
    "            \n",
    "        data_idx_list.append(_str)\n",
    "\n",
    "    df['data_label'] = data_idx_list\n",
    "    \n",
    "def vfd_data_label_sim_write(df):\n",
    "    data_idx = 0\n",
    "    data_idx_list = []\n",
    "    dsets = list(set(df['file_no']))\n",
    "    curr_dset = dsets.pop(0)\n",
    "    dset_changed = False\n",
    "    for index, row in df.iterrows():\n",
    "        _pat = row['op_type']\n",
    "        \n",
    "        if _pat =='data':\n",
    "            data_idx_list.append(f\"vfd-{_pat}-{curr_dset}-{data_idx}\")\n",
    "            data_idx+=1\n",
    "        else:\n",
    "            data_idx_list.append(f\"vfd-{_pat}-{curr_dset}\")\n",
    "            \n",
    "        if row['file_no'] != curr_dset:\n",
    "            data_idx = 0\n",
    "            curr_dset = dsets.pop(0)\n",
    "\n",
    "    df['data_label'] = data_idx_list\n",
    "\n",
    "def fill_dset_offset(dfvol, dfvfd, fno2dset_dict={1:'contact_map', 2:'point_cloud'}):\n",
    "    dsets_no = list(fno2dset_dict.keys())\n",
    "    offset_dict = {}\n",
    "    for no in dsets_no:\n",
    "        # find first dataset starting offset\n",
    "        # vfd_sdf[vfd_sdf['data_label'] == f'vfd-data-{dset}-0'].iloc[0]['start_addr']\n",
    "        \n",
    "        idx = dfvfd.data_label.str.match(f'vfd-data-{no}-0').idxmax()\n",
    "        START_OFFSET = dfvfd.iloc[idx]['start_addr']\n",
    "        offset_dict[fno2dset_dict[no]] = START_OFFSET\n",
    "    \n",
    "    dfvol['offset'] = dfvol['dset_name'].map(offset_dict)\n",
    "    return dfvol\n",
    "\n",
    "def hard_code_start_addr(dfvol, dfvfd):\n",
    "    # # hardcode starting address\n",
    "    dfvol = fill_dset_offset(dfvol, dfvfd)\n",
    "    # starting address of blob\n",
    "    idx_vfd = dfvfd.op_type.str.match('data').idxmax() # obtain from first vfd access \n",
    "    BLOB_START_ADDR = dfvfd.iloc[idx_vfd]['access_size'] +  dfvfd.iloc[idx_vfd]['start_addr'] \n",
    "    idx_vol = dfvol.func_name.str.match('H5VLblob_put').idxmax()\n",
    "    dfvol.loc[idx_vol, ['logical_addr']] = BLOB_START_ADDR\n",
    "    # print(dfvol.iloc[idx_vol])\n",
    "    \n",
    "    # starting address of dset\n",
    "    dset_idx = dfvol.index[dfvol['func_name'] == 'H5VLdataset_write'].tolist()\n",
    "    \n",
    "    for dsi in dset_idx:\n",
    "        dfvol.loc[dsi, ['logical_addr']] = dfvol.iloc[dsi]['offset']\n",
    "    \n",
    "    return dfvol\n",
    "\n",
    "# # bfill_cols = ['layout', 'type_size', 'n_elements', 'dimension_cnt', 'dimensions', 'dset_name', 'file_intent']\n",
    "\n",
    "add_all_op_type(vol_sdf,vfd_sdf)\n",
    "# # add data_label for simulation\n",
    "vol_sdf['dset_name'] = vol_sdf['dset_name'].fillna(method='bfill')\n",
    "vol_sdf['dset_name'] = vol_sdf['dset_name'].fillna(method='ffill')\n",
    "\n",
    "vfd_data_label_sim_write(vfd_sdf)\n",
    "vol_data_label(vol_sdf)\n",
    "\n",
    "vol_sdf = hard_code_start_addr(vol_sdf,vfd_sdf)\n",
    "\n",
    "# check_empty(vfd_sdf)\n",
    "\n",
    "df_to_csv(vol_sdf,fsim,suffix='-vol')\n",
    "df_to_csv(vfd_sdf,fsim,suffix='-vfd')\n",
    "\n",
    "# # TODO: use parquet for faster load in later analysis\n",
    "# df_to_parquet(vol_sdf,fsim,suffix='-vol') \n",
    "# df_to_parquet(vfd_sdf,fsim,suffix='-vfd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fagg='../save_outputs/vol-vfd/prov-vfd-agg.log'\n",
    "\n",
    "vol_agg_rec, vfd_agg_rec = split_vfd_vol_rec(fagg)\n",
    "\n",
    "vol_adf = rec_to_df(vol_agg_rec)\n",
    "vfd_adf = rec_to_df(vfd_agg_rec)\n",
    "\n",
    "# add category, split read write based on filename\n",
    "vol_adf['cat'] = np.where(vol_adf['file_name']== 'aggregate.h5', 'agg-write', 'agg-read')\n",
    "vfd_adf['cat'] = np.where(vfd_adf['file_name']== 'aggregate.h5', 'agg-write', 'agg-read')\n",
    "\n",
    "# add columns for mapping\n",
    "vfd_adf['start_addr'] = vfd_adf['start_addr'].fillna(-1)\n",
    "vfd_adf['access_size'] = vfd_adf['access_size'].fillna(0)\n",
    "vfd_adf['next_addr'] = vfd_adf['start_addr'] + vfd_adf['access_size']\n",
    "vfd_adf['next_addr'] = vfd_adf['next_addr'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in aggregation data\n",
    "\n",
    "def vfd_op_data_label_agg_read(df, dset_offsets):\n",
    "\n",
    "    data_label_list = []\n",
    " \n",
    "    curr_dset = 1\n",
    "    next_offset = dset_offsets[curr_dset]\n",
    "    dset_changed = False\n",
    "    \n",
    "    data_idx = 1\n",
    "    last_addr = -1 # vfd has no -1 address\n",
    "    pattern_list = []\n",
    "    \n",
    "    for index, row in df.iterrows(): #islice(df.iterrows(), start_idx, None):\n",
    "        if row['mem_type'] == \"H5FD_MEM_DRAW\":\n",
    "            if row['access_size'] == 4096:\n",
    "                last_addr = row['next_addr']\n",
    "                data_label_list.append(f'vfd-data-{curr_dset}-{data_idx}')\n",
    "                pattern_list.append('loc')\n",
    "            elif row['start_addr'] == last_addr:\n",
    "                data_label_list.append(f'vfd-data-{curr_dset}-{data_idx}')\n",
    "                pattern_list.append('data')\n",
    "                data_idx+=1\n",
    "                last_addr = 0\n",
    "            else:\n",
    "                data_label_list.append(f'vfd-data-{curr_dset}-{0}') # first access\n",
    "                pattern_list.append('data')\n",
    "        else:\n",
    "            data_label_list.append(f'vfd-meta-{curr_dset}')\n",
    "            last_addr = row['next_addr']\n",
    "            pattern_list.append('meta')\n",
    "            \n",
    "        if row['start_addr'] == next_offset:\n",
    "            data_idx = 0\n",
    "            curr_dset+=1\n",
    "            if len(dset_offsets) <= curr_dset:\n",
    "                next_offset = 0\n",
    "            else:\n",
    "                next_offset = dset_offsets[curr_dset]\n",
    "            dset_changed = True\n",
    "    \n",
    "    data_label_list[-1] = f'vfd-data-{curr_dset}-{data_idx}' # last idx for point_cloud\n",
    "    \n",
    "    df['data_label'] = data_label_list\n",
    "    df['op_type'] = pattern_list\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# add mapping for data_label\n",
    "vol_adf_read = vol_adf[vol_adf['cat'] == 'agg-read'].copy().reset_index()\n",
    "vfd_adf_read = vfd_adf[vfd_adf['cat'] == 'agg-read'].copy().reset_index()\n",
    "\n",
    "# get dset offsets from vol df\n",
    "dset_offsets = list(set(vol_adf_read[vol_adf_read['func_name'] == 'H5VLdataset_read']['offset']))\n",
    "# print(dset_offsets)\n",
    "\n",
    "# # add data_label for vol\n",
    "vol_adf_read['dset_name'] = vol_adf_read['dset_name'].fillna(method='bfill')\n",
    "vol_adf_read['dset_name'] = vol_adf_read['dset_name'].fillna(method='ffill')\n",
    "add_vol_op_type(vol_adf_read)\n",
    "vol_data_label(vol_adf_read)\n",
    "\n",
    "\n",
    "## add data_label for vfd\n",
    "vfd_adf_read = vfd_op_data_label_agg_read(vfd_adf_read,dset_offsets)\n",
    "# # df_to_csv(vfd_adf_read,fagg,suffix='-vfd-read')\n",
    "\n",
    "\n",
    "df_to_csv(vol_adf_read,fagg,suffix='-vol-read')\n",
    "df_to_csv(vfd_adf_read,fagg,suffix='-vfd-read')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create lables for AGG_WRITE phase\n",
    "1. create pattern for all 16B, 3KB I/O, and metadata and H5FD_MEM_LHEAP\n",
    "2. create data_label for all 3KB I/O\n",
    "3. map data_label with start_addr-3KB to end_addr-16B\n",
    "** currently dataset is seperated with I/O size that is characteristic of point_cloud. Cannot work with other datasets for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def vfd_data_pattern_agg_write(df):\n",
    "    op_type_list = []\n",
    "    # tmp_row = pd.DataFrame(columns=list(df.columns))\n",
    "    for index, row in df.iterrows():\n",
    "        if row['mem_type'] == 'H5FD_MEM_DRAW':\n",
    "            if row['access_size'] == 16:\n",
    "                op_type_list.append('loc')\n",
    "            else:\n",
    "                op_type_list.append('data')\n",
    "        elif row['mem_type'] == 'H5FD_MEM_LHEAP':\n",
    "            op_type_list.append('lheap')\n",
    "        else:\n",
    "            op_type_list.append('meta')\n",
    "    \n",
    "    df['op_type'] = op_type_list\n",
    "    return df\n",
    "\n",
    "        \n",
    "def vfd_data_label_agg_write(df, PC_IO_SIZE):\n",
    "    # TODO: needs improvement\n",
    "\n",
    "    data_idx = 0\n",
    "    pc_idx = 0\n",
    "    lheap_idx = 0\n",
    "        \n",
    "    data_label_list = []\n",
    "    loc_map = {}\n",
    "    data_label_map  = {}\n",
    "    \n",
    "    first_chunk_index = 0 # record index at dataframe\n",
    "    prev_address = df[df['op_type'] == 'data'].iloc[0]['start_addr'] # first data address\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        _pat = row['op_type']\n",
    "        if _pat == 'data':\n",
    "            if row['access_size'] == PC_IO_SIZE:\n",
    "                data_label_list.append(f'vfd-data-point_cloud-{pc_idx}')\n",
    "                pc_idx +=1\n",
    "            else:\n",
    "                _str = f'vfd-data-contact_map-{data_idx}'\n",
    "                data_label_list.append(_str)\n",
    "                if first_chunk_index == 0:\n",
    "                    if row['start_addr'] != prev_address:\n",
    "                        first_chunk_index = index\n",
    "                else:\n",
    "                    data_label_map[row['start_addr']] = _str\n",
    "                data_idx +=1\n",
    "                prev_address = row['next_addr']\n",
    "                \n",
    "        elif _pat == 'meta':\n",
    "            data_label_list.append(f'vfd-{_pat}-1')\n",
    "        elif _pat == 'lheap':\n",
    "            data_label_list.append(f'vfd-{_pat}-1-{lheap_idx}')\n",
    "            lheap_idx+=1\n",
    "        else: \n",
    "            if loc_map:\n",
    "                data_label_list.append('0')\n",
    "            else:\n",
    "                data_label_list.append(f'vfd-lheap-1')\n",
    "            loc_map[row['next_addr']] = int(row['io_access_idx'])\n",
    "    \n",
    "    # map some 16B with data_label\n",
    "    for k in data_label_map.keys():\n",
    "        data_label_list[loc_map[k]] = data_label_map[k]\n",
    "    \n",
    "    # find and map the rest 16B with initial chunk\n",
    "    indices = [i for i, x in enumerate(data_label_list) if x == '0']\n",
    "    data_idx = 0\n",
    "    for idx in indices:\n",
    "        data_label_list[idx] = f'vfd-data-contact_map-{data_idx}'\n",
    "        data_idx+=1\n",
    "\n",
    "    df['data_label'] = data_label_list\n",
    "\n",
    "def get_point_cloud_io_size(dfvol):\n",
    "    idx = dfvol.dset_name.str.match('point_cloud').idxmax()\n",
    "    # dims = ast.literal_eval(lastrow['dimensions'])\n",
    "    first_dim = dfvol.iloc[idx]['dimensions'][0] #lastrow['dimensions'][idx]\n",
    "    access_size = dfvol.iloc[idx]['access_size']\n",
    "    io_size = access_size / first_dim\n",
    "    return io_size\n",
    "\n",
    "def hard_code_start_addr_agg(dfvol, dfvfd):\n",
    "    # # hardcode starting address\n",
    "    for dset in ['contact_map', 'point_cloud']:\n",
    "        idx_vfd = dfvfd.data_label.str.match(f'vfd-data-{dset}-0').idxmax()\n",
    "        START_ADDR = dfvfd.iloc[idx_vfd]['start_addr']\n",
    "        \n",
    "        idx_vol = dfvol.data_label.str.match(f'vol-data-{dset}-0').idxmax()\n",
    "        dfvol.loc[idx_vol, ['logical_addr']] = START_ADDR\n",
    "\n",
    "        idx_vol = dfvol.data_label.str.match(f'vol-data-{dset}-1').idxmax()\n",
    "        if idx_vol:\n",
    "            dfvol.loc[idx_vol, ['logical_addr']] = START_ADDR\n",
    "    \n",
    "    return dfvol\n",
    "\n",
    "pattern_dict = { # for notes only now\n",
    "    'mem_type' : {\n",
    "        'H5FD_MEM_LHEAP' : 'lheap',\n",
    "        'H5FD_MEM_DRAW' : { 'access_size':{ 16 : 'loc', 'others' : 'data'} },\n",
    "        'H5FD_MEM_OHDR': 'meta',\n",
    "        'H5FD_MEM_SUPER': 'meta'\n",
    "    }\n",
    "}\n",
    "\n",
    "vfd_adf_write = vfd_adf[vfd_adf['cat'] == 'agg-write'].copy().reset_index()\n",
    "vol_adf_write = vol_adf[vol_adf['cat'] == 'agg-write'].copy().reset_index()\n",
    "\n",
    "# check_empty(vol_adf_write)\n",
    "\n",
    "# add data_label for vol\n",
    "vol_adf_write['dset_name'] = vol_adf_write['dset_name'].fillna(method='bfill')\n",
    "vol_adf_write['dset_name'] = vol_adf_write['dset_name'].fillna(method='ffill')\n",
    "add_vol_op_type(vol_adf_write)\n",
    "vol_data_label(vol_adf_write)\n",
    "\n",
    "vfd_adf_write = vfd_data_pattern_agg_write(vfd_adf_write)\n",
    "PC_IO_SIZE = get_point_cloud_io_size(vol_adf_write)\n",
    "vfd_data_label_agg_write(vfd_adf_write, PC_IO_SIZE)\n",
    "\n",
    "\n",
    "# get logical address from vfd to vol\n",
    "vol_adf_write = hard_code_start_addr_agg(vol_adf_write,vfd_adf_write)\n",
    "\n",
    "df_to_csv(vol_adf_write,fagg,suffix='-vol-write')\n",
    "df_to_csv(vfd_adf_write,fagg,suffix='-vfd-write')\n",
    "# df_to_parquet(vol_adf_write,fagg,suffix='-vol-write')\n",
    "# df_to_parquet(vfd_adf_write,fagg,suffix='-vfd-write')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Aug  1 2022, 17:00:54) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b91b7dd4d8a7b6a913637a2227eb724977af8647a7e6e9f1d2538a8f2b15605"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
