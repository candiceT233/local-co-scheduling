{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import traceback\n",
    "\n",
    "from itertools import islice\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import math\n",
    "\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "# pd.set_option('display.max_columns', 4)\n",
    "# pd.set_option('display.width', 1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Steps\n",
    "1. splits each logs into 3 parts, sim_write , agg_read , agg_write, \n",
    "2. splits each part into 2 dataframe, df_vol and df_vfd \n",
    "3. apply analysis based mapping and merge df_vol and df_vfd\n",
    "4. saved each df_sim_merged, df_agg_read_merged, df_agg_write_merged into parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_empty(df,p=True):\n",
    "    nan_df = df[df.isna().any(axis=1)].copy()\n",
    "    if not nan_df.empty:\n",
    "        if p:\n",
    "            print(f\"NaN rows: {nan_df}\") # check any nan\n",
    "        return list(nan_df.index)\n",
    "    \n",
    "    null_df = df[df.isnull().any(axis=1)].copy()\n",
    "    if not null_df.empty:\n",
    "        if p:\n",
    "            print(f\"NULL rows: {null_df}\") # check any null\n",
    "        return list(null_df.index)\n",
    "    \n",
    "def rec_to_df(records):\n",
    "    df = pd.DataFrame.from_dict(records,orient='index')\n",
    "    df.replace('/mnt/ssd/mtang11/','',regex=True, inplace=True)\n",
    "    df.replace('molecular_dynamics_runs/stage0000/','',regex=True, inplace=True)\n",
    "    \n",
    "    # df['hash_id']= df['hash_id'].astype(str)\n",
    "    # df['hash_id'] = map(lambda x: x.encode('base64','strict'), df['hash_id'])\n",
    "    if 'logical_addr' in df.columns:\n",
    "        df['logical_addr'] = df['logical_addr'].fillna(-1.0) #.astype(int)\n",
    "    if 'access_size' in df.columns:\n",
    "        df['access_size'] = df['access_size'].fillna(0)\n",
    "    #df['logical_addr'] = df['logical_addr'].astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def df_to_csv(df,file_name,suffix=''):\n",
    "    # df.hash_id=df.hash_id.astype('category').cat.codes\n",
    "    # out_csv=file_name.replace('prov-vfd-','')\n",
    "    out_csv=file_name.replace('.log',f'{suffix}.csv')\n",
    "    df.to_csv(out_csv,index=False)\n",
    "\n",
    "def df_to_parquet(df,file_name,suffix=''):\n",
    "    out_parquet=file_name.replace('.log',f'{suffix}.parquet')\n",
    "    try:\n",
    "        df.to_parquet(out_parquet, engine='pyarrow') # compression='gzip'\n",
    "        # pyarrow has error with streamlit, downgrade to streamlit==0.84.2\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "\n",
    "def print_list_diff(list1, list2):\n",
    "    if list1 == list2:\n",
    "        print(\"same lists\")\n",
    "    else:\n",
    "        print(\"different lists\\nindex:\\t[list1]\\t[list2]\")\n",
    "        for index, (first, second) in enumerate(zip(list1, list2)):\n",
    "            if first != second:\n",
    "                # print(index, first, second)\n",
    "                print(f\"{index}:\\t{first}]\\t[{second}]\")\n",
    "\n",
    "\n",
    "def split_vfd_vol_rec(fname,mode='r'):\n",
    "    vol_r_ops = ['H5VLdataset_read', 'H5VLblob_get', 'H5FD__hermes_read']\n",
    "    vol_w_ops = ['H5VLdataset_write', 'H5VLblob_put', 'H5FD__hermes_write']\n",
    "    other_ops = ['_get','_create', '_close', '_open',]\n",
    "\n",
    "    vol_rec = {}\n",
    "    vfd_rec = {}\n",
    "    vol_idx = 0\n",
    "    vfd_idx = 0\n",
    "    with open(fname, mode) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                rec = ast.literal_eval(line)\n",
    "                \n",
    "                if any(op in rec['func_name'] for op in vol_r_ops):\n",
    "                    if 'hermes' in rec['func_name']:\n",
    "                        vfd_rec[vfd_idx] = rec\n",
    "                        vfd_rec[vfd_idx]['operation'] = 'write'\n",
    "                        vfd_idx+=1\n",
    "                    else:\n",
    "                        vol_rec[vol_idx] = rec\n",
    "                        vol_rec[vol_idx]['operation'] = 'read'\n",
    "                        vol_idx+=1\n",
    "                elif any(op in rec['func_name'] for op in vol_w_ops):\n",
    "                    if 'hermes' in rec['func_name']:\n",
    "                        vfd_rec[vfd_idx] = rec\n",
    "                        vfd_rec[vfd_idx]['operation'] = 'write'\n",
    "                        vfd_idx+=1\n",
    "                    else:\n",
    "                        vol_rec[vol_idx] = rec\n",
    "                        vol_rec[vol_idx]['operation'] = 'write'\n",
    "                        vol_idx+=1          \n",
    "                # not record other_ops\n",
    "                elif any(op in rec['func_name'] for op in other_ops):\n",
    "                    _suffix = rec['func_name'].split('_')[-1]\n",
    "                    if 'hermes' in rec['func_name']:\n",
    "                        vfd_rec[vfd_idx] = rec\n",
    "                        vfd_rec[vfd_idx]['operation'] = _suffix\n",
    "                        vfd_idx+=1\n",
    "                    else:\n",
    "                        vol_rec[vol_idx] = rec\n",
    "                        vol_rec[vol_idx]['operation'] = _suffix\n",
    "                        vol_idx+=1\n",
    "                    \n",
    "            except:\n",
    "                print(\"Erro line:\")\n",
    "                print(line)\n",
    "                # break\n",
    "    return vol_rec, vfd_rec\n",
    "\n",
    "def add_all_op_type(dfvol, dfvfd, vol_map={}, vfd_map={}):\n",
    "    if bool(vol_map) == False:\n",
    "        vol_map={'read':'data', 'write': 'data'}\n",
    "        vfd_map={'H5FD_MEM_DRAW':'data', 'H5FD_MEM_LHEAP': 'lheap'}\n",
    "\n",
    "    dfvol['op_type'] = dfvol['operation'].map(vol_map)\n",
    "    dfvol['op_type'] = dfvol['op_type'].fillna('meta')\n",
    "    \n",
    "    dfvfd['mem_type'] = dfvfd['mem_type'].fillna('H5FD_MEM_NTYPES')\n",
    "    dfvfd['op_type'] = dfvfd['mem_type'].map(vfd_map)\n",
    "    dfvfd['op_type'] = dfvfd['op_type'].fillna('meta')\n",
    "\n",
    "def add_vol_op_type(dfvol, vol_map={}):\n",
    "    if bool(vol_map) == False:\n",
    "        vol_map={'read':'data', 'write': 'data'}\n",
    "\n",
    "    dfvol['op_type'] = dfvol['operation'].map(vol_map)\n",
    "    dfvol['op_type'] = dfvol['op_type'].fillna('meta')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['data_label_vol', 'io_access_idx_vol', 'dset_name', 'access_size_vol',\n",
      "       'op_type_vol', 'logical_addr', 'operation_vol', 'n_elements',\n",
      "       'dimension_cnt', 'dimensions', 'file_intent', 'layout', 'time(us)_vol',\n",
      "       'logical_addr_map', 'data_label_vfd', 'io_access_idx_vfd',\n",
      "       'access_size_vfd', 'op_type_vfd', 'logical_addr_vfd', 'file_name',\n",
      "       'time(us)_vfd', 'operation_vfd'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# read in simulation data\n",
    "def vol_data_label(df):\n",
    "    data_idx_list = []\n",
    "    dsets = list(set(df['file_no']))\n",
    "    for index, row in df.iterrows():\n",
    "        _pat = row['op_type']\n",
    "        _dset = row['dset_name']\n",
    "        _str = f\"vol-{_pat}-{_dset}\"\n",
    "        if _pat =='data':\n",
    "            _str = f\"vol-{_pat}-{_dset}-{row['io_access_idx']}\"\n",
    "            \n",
    "        data_idx_list.append(_str)\n",
    "\n",
    "    df['data_label'] = data_idx_list\n",
    "    \n",
    "def vfd_data_label_sim_write(df):\n",
    "    data_idx = 0\n",
    "    data_idx_list = []\n",
    "    dsets = list(set(df['file_no']))\n",
    "    curr_dset = dsets.pop(0)\n",
    "    dset_changed = False\n",
    "    for index, row in df.iterrows():\n",
    "        _pat = row['op_type']\n",
    "        \n",
    "        if _pat =='data':\n",
    "            data_idx_list.append(f\"vfd-{_pat}-{curr_dset}-{data_idx}\")\n",
    "            data_idx+=1\n",
    "        else:\n",
    "            data_idx_list.append(f\"vfd-{_pat}-{curr_dset}\")\n",
    "            \n",
    "        if row['file_no'] != curr_dset:\n",
    "            data_idx = 0\n",
    "            curr_dset = dsets.pop(0)\n",
    "\n",
    "    df['data_label'] = data_idx_list\n",
    "\n",
    "def fill_dset_offset(dfvol, dfvfd, fno2dset_dict={1:'contact_map', 2:'point_cloud'}):\n",
    "    dsets_no = list(fno2dset_dict.keys())\n",
    "    offset_dict = {}\n",
    "    for no in dsets_no:\n",
    "        # find first dataset starting offset\n",
    "        # vfd_sdf[vfd_sdf['data_label'] == f'vfd-data-{dset}-0'].iloc[0]['start_addr']\n",
    "        \n",
    "        idx = dfvfd.data_label.str.match(f'vfd-data-{no}-0').idxmax()\n",
    "        START_OFFSET = dfvfd.iloc[idx]['start_addr']\n",
    "        offset_dict[fno2dset_dict[no]] = START_OFFSET\n",
    "    \n",
    "    dfvol['offset'] = dfvol['dset_name'].map(offset_dict)\n",
    "    return dfvol\n",
    "\n",
    "def hard_code_start_addr(dfvol, dfvfd):\n",
    "    # # hardcode starting address\n",
    "    dfvol = fill_dset_offset(dfvol, dfvfd)\n",
    "    # starting address of blob\n",
    "    idx_vfd = dfvfd.op_type.str.match('data').idxmax() # obtain from first vfd access \n",
    "    BLOB_START_ADDR = dfvfd.iloc[idx_vfd]['access_size'] +  dfvfd.iloc[idx_vfd]['start_addr'] \n",
    "    idx_vol = dfvol.func_name.str.match('H5VLblob_put').idxmax()\n",
    "    dfvol.loc[idx_vol, ['logical_addr']] = BLOB_START_ADDR\n",
    "    # print(dfvol.iloc[idx_vol])\n",
    "    \n",
    "    # starting address of dset\n",
    "    dset_idx = dfvol.index[dfvol['func_name'] == 'H5VLdataset_write'].tolist()\n",
    "    \n",
    "    for dsi in dset_idx:\n",
    "        dfvol.loc[dsi, ['logical_addr']] = dfvol.iloc[dsi]['offset']\n",
    "    \n",
    "    return dfvol\n",
    "\n",
    "def merge_df_sim(dfvol, dfvfd):\n",
    "    df_sim2vol = dfvol[['data_label', 'io_access_idx','dset_name', 'access_size', 'op_type', 'logical_addr',\n",
    "                        'operation', 'n_elements','dimension_cnt','dimensions','file_intent','layout','time(us)']].copy()\n",
    "    vol_idx2addr_map = dict(zip(df_sim2vol['data_label'], df_sim2vol['logical_addr']))\n",
    "    df_sim2vol['logical_addr_map'] = df_sim2vol['logical_addr'].astype(int)\n",
    "\n",
    "    df_vfd2res = dfvfd[['data_label', 'io_access_idx','access_size', 'op_type','start_addr',\n",
    "                        'file_name', 'time(us)','operation']].copy()\n",
    "    \n",
    "    # df_vol2res.rename(columns={'start_addr':'logical_addr'}, inplace=True)\n",
    "    df_vfd2res['start_addr'] = df_vfd2res['start_addr'].fillna(0)\n",
    "    df_vfd2res['logical_addr_map'] = df_vfd2res['start_addr'].astype(int)\n",
    "    \n",
    "    # modify all meta mapping address and info\n",
    "    df_sim2vol.loc[df_sim2vol['op_type'] == 'meta', 'logical_addr_map'] = 0\n",
    "    df_vfd2res.loc[df_vfd2res['op_type'] == 'meta', 'logical_addr_map'] = 0\n",
    "    \n",
    "    df_vfd2res.rename(columns={'start_addr':'logical_addr_vfd'}, inplace=True)\n",
    "    df_merged = df_sim2vol.merge(df_vfd2res, how='inner', on=['logical_addr_map'], suffixes=['_vol','_vfd'])\n",
    "\n",
    "    # populate file_name\n",
    "    df_merged['file_name'] = df_merged['file_name'].fillna('task0000/residue_100.h5')\n",
    "    # df_merged['io_access_idx_vol'] = df_merged['io_access_idx_vol'].fillna(-1)\n",
    "    # df_merged['access_size_vol'] = df_merged['access_size_vol'].fillna(0)\n",
    "    # df_merged['access_size_vol'] = df_merged['access_size_vol'].replace(np.nan, 0)\n",
    "    \n",
    "    return df_merged, vol_idx2addr_map\n",
    "\n",
    "fsim='../save_outputs/vol-vfd/prov-vfd-sim.log'\n",
    "mode='r'\n",
    "\n",
    "vol_sim_rec, vfd_sim_rec = split_vfd_vol_rec(fsim)\n",
    "\n",
    "vol_sdf = rec_to_df(vol_sim_rec)\n",
    "vfd_sdf = rec_to_df(vfd_sim_rec)\n",
    "\n",
    "# add category\n",
    "vol_sdf['cat'] = 'sim-write'\n",
    "vfd_sdf['cat'] = 'sim-write'\n",
    "\n",
    "# # bfill_cols = ['layout', 'type_size', 'n_elements', 'dimension_cnt', 'dimensions', 'dset_name', 'file_intent']\n",
    "\n",
    "add_all_op_type(vol_sdf,vfd_sdf)\n",
    "# # add data_label for simulation\n",
    "vol_sdf['dset_name'] = vol_sdf['dset_name'].fillna(method='bfill')\n",
    "vol_sdf['dset_name'] = vol_sdf['dset_name'].fillna(method='ffill')\n",
    "\n",
    "vfd_data_label_sim_write(vfd_sdf)\n",
    "vol_data_label(vol_sdf)\n",
    "vol_sdf.loc[vol_sdf['op_type'] == 'meta', 'io_access_idx'] = -1\n",
    "# check_empty(vol_sdf)\n",
    "\n",
    "vol_sdf = hard_code_start_addr(vol_sdf,vfd_sdf)\n",
    "\n",
    "# check_empty(vfd_sdf)\n",
    "df_to_csv(vol_sdf,fsim,suffix='-vol')\n",
    "df_to_csv(vfd_sdf,fsim,suffix='-vfd')\n",
    "\n",
    "df_merged, vol_idx2addr_map = merge_df_sim(vol_sdf,vfd_sdf)\n",
    "print(df_merged.columns)\n",
    "df_to_csv(df_merged,fsim,suffix='-merged') # output merged to csv\n",
    "\n",
    "# # TODO: use parquet for faster load in later analysis\n",
    "# df_to_parquet(df_merged,fsim,suffix='-merged')\n",
    "df_to_csv(df_merged,fsim,suffix='-merged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['data_label_vol', 'io_access_idx_vol', 'dset_name', 'access_size_vol',\n",
      "       'op_type_vol', 'logical_addr_vol', 'operation_vol', 'n_elements',\n",
      "       'dimension_cnt', 'dimensions', 'file_intent', 'layout', 'time(us)_vol',\n",
      "       'logical_addr_map', 'data_label_vfd', 'io_access_idx_vfd',\n",
      "       'access_size_vfd', 'op_type_vfd', 'next_addr', 'logical_addr_vfd',\n",
      "       'file_name', 'time(us)_vfd', 'operation_vfd'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# map only aggregation read\n",
    "\n",
    "# read in aggregation data\n",
    "def vfd_op_data_label_agg_read(df, dset_offsets):\n",
    "\n",
    "    data_label_list = []\n",
    " \n",
    "    curr_dset = 1\n",
    "    next_offset = dset_offsets[curr_dset]\n",
    "    dset_changed = False\n",
    "    \n",
    "    data_idx = 1\n",
    "    last_addr = -1 # vfd has no -1 address\n",
    "    pattern_list = []\n",
    "    \n",
    "    for index, row in df.iterrows(): #islice(df.iterrows(), start_idx, None):\n",
    "        if row['mem_type'] == \"H5FD_MEM_DRAW\":\n",
    "            if row['access_size'] == 4096:\n",
    "                last_addr = row['next_addr']\n",
    "                data_label_list.append(f'vfd-data-{curr_dset}-{data_idx}')\n",
    "                pattern_list.append('loc')\n",
    "            elif row['start_addr'] == last_addr:\n",
    "                data_label_list.append(f'vfd-data-{curr_dset}-{data_idx}')\n",
    "                pattern_list.append('data')\n",
    "                data_idx+=1\n",
    "                last_addr = 0\n",
    "            else:\n",
    "                data_label_list.append(f'vfd-data-{curr_dset}-{0}') # first access\n",
    "                pattern_list.append('data')\n",
    "        else:\n",
    "            data_label_list.append(f'vfd-meta-{curr_dset}')\n",
    "            last_addr = row['next_addr']\n",
    "            pattern_list.append('meta')\n",
    "            \n",
    "        if row['start_addr'] == next_offset:\n",
    "            data_idx = 0\n",
    "            curr_dset+=1\n",
    "            if len(dset_offsets) <= curr_dset:\n",
    "                next_offset = 0\n",
    "            else:\n",
    "                next_offset = dset_offsets[curr_dset]\n",
    "            dset_changed = True\n",
    "    \n",
    "    data_label_list[-1] = f'vfd-data-{curr_dset}-{data_idx}' # last idx for point_cloud\n",
    "    \n",
    "    df['data_label'] = data_label_list\n",
    "    df['op_type'] = pattern_list\n",
    "    \n",
    "    return df\n",
    "\n",
    "def merge_df_agg_read(dfvol, dfvfd,vol_idx2addr_map):\n",
    "    df_agg2vol = dfvol[['data_label', 'io_access_idx','dset_name', 'access_size', 'op_type','logical_addr',\n",
    "                        'operation', 'n_elements','dimension_cnt','dimensions','file_intent','layout','time(us)']].copy()\n",
    "    df_vfd2res = dfvfd[['data_label', 'io_access_idx','access_size', 'op_type', 'next_addr','start_addr',\n",
    "                        'file_name','time(us)','operation']].copy()\n",
    "\n",
    "    # df_agg2vol['logical_addr'] = df_agg2vol['logical_addr'].astype(int)\n",
    "    \n",
    "    # address map for vol\n",
    "    df_agg2vol['logical_addr_map'] = df_agg2vol['data_label'].map(vol_idx2addr_map)\n",
    "    df_agg2vol.loc[df_agg2vol['op_type'] == 'meta', 'logical_addr_map'] = 0 # all meta map to 0\n",
    "    \n",
    "    # split by op_type\n",
    "    df_vfd2res_meta = df_vfd2res[df_vfd2res['op_type'] == 'meta'].copy() # use 0 for meta\n",
    "    df_vfd2res_loc = df_vfd2res[df_vfd2res['op_type'] == 'loc'].copy() # this match vol with start_addr\n",
    "    df_vfd2res_data = df_vfd2res[df_vfd2res['op_type'] == 'data'].copy() # this match loc with next_addr\n",
    "    \n",
    "    # address map for vfd-meta\n",
    "    df_vfd2res_meta['logical_addr_map'] = 0\n",
    "    df_vfd2res_loc['logical_addr_map'] = df_vfd2res_loc['start_addr']\n",
    "    \n",
    "    # address map for vfd-data\n",
    "    df_vfd2res_data_map  = dict(zip(df_vfd2res_loc['next_addr'], df_vfd2res_loc['start_addr']))\n",
    "    df_vfd2res_data['logical_addr_map'] = df_vfd2res_data['start_addr'].map(df_vfd2res_data_map)\n",
    "    \n",
    "    nan_idx = check_empty(df_vfd2res_data,p=False)\n",
    "    for idx in nan_idx:\n",
    "        df_vfd2res_data.loc[idx, ['logical_addr_map']] = df_vfd2res.iloc[idx]['start_addr']\n",
    "    \n",
    "    # # rename and conver types\n",
    "    # df_vol2res_meta['logical_addr_map'] = df_vol2res_meta['start_addr']\n",
    "    # df_vol2res_loc['logical_addr_map'] = df_vol2res_loc['start_addr']\n",
    "    \n",
    "    df_vfd2res = pd.concat([df_vfd2res_meta,df_vfd2res_loc, df_vfd2res_data], axis=0) #.set_index('io_access_idx')\n",
    "\n",
    "    # # df_vol2res_data['logical_addr_map'] = df_vol2res_data['logical_addr_map'].astype(int)\n",
    "    df_vfd2res.rename(columns={'start_addr':'logical_addr'}, inplace=True)\n",
    "    df_vfd2res['logical_addr'] = df_vfd2res['logical_addr'].replace(-1,0)\n",
    "    \n",
    "    df_merged = df_agg2vol.merge(df_vfd2res, how='inner', on=['logical_addr_map'], suffixes=['_vol','_vfd'])\n",
    "    # df_merged['access_size_vol'] = df_merged['access_size_vol'].fillna(0)\n",
    "    # df_merged['access_size_vol'] = df_merged['access_size_vol'].replace(np.nan, 0)\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "fagg='../save_outputs/vol-vfd/prov-vfd-agg.log'\n",
    "\n",
    "vol_agg_rec, vfd_agg_rec = split_vfd_vol_rec(fagg)\n",
    "\n",
    "vol_adf = rec_to_df(vol_agg_rec)\n",
    "vfd_adf = rec_to_df(vfd_agg_rec)\n",
    "\n",
    "# add category, split read write based on filename\n",
    "vol_adf['cat'] = np.where(vol_adf['file_name']== 'aggregate.h5', 'agg-write', 'agg-read')\n",
    "vfd_adf['cat'] = np.where(vfd_adf['file_name']== 'aggregate.h5', 'agg-write', 'agg-read')\n",
    "\n",
    "# add columns for mapping\n",
    "vfd_adf['start_addr'] = vfd_adf['start_addr'].fillna(-1.0)\n",
    "vfd_adf['access_size'] = vfd_adf['access_size'].fillna(0.0)\n",
    "vfd_adf['next_addr'] = vfd_adf['start_addr'] + vfd_adf['access_size']\n",
    "vfd_adf['next_addr'] = vfd_adf['next_addr'].astype(int)\n",
    "\n",
    "# add mapping for data_label\n",
    "vol_adf_read = vol_adf[vol_adf['cat'] == 'agg-read'].copy().reset_index()\n",
    "vfd_adf_read = vfd_adf[vfd_adf['cat'] == 'agg-read'].copy().reset_index()\n",
    "\n",
    "# get dset offsets from vol df\n",
    "dset_offsets = list(set(vol_adf_read[vol_adf_read['func_name'] == 'H5VLdataset_read']['offset']))\n",
    "# print(dset_offsets)\n",
    "\n",
    "# # add data_label for vol\n",
    "vol_adf_read['dset_name'] = vol_adf_read['dset_name'].fillna(method='bfill')\n",
    "vol_adf_read['dset_name'] = vol_adf_read['dset_name'].fillna(method='ffill')\n",
    "add_vol_op_type(vol_adf_read)\n",
    "vol_data_label(vol_adf_read)\n",
    "vol_adf_read.loc[vol_adf_read['op_type'] == 'meta', 'io_access_idx'] = -1\n",
    "\n",
    "## add data_label for vfd\n",
    "vfd_adf_read = vfd_op_data_label_agg_read(vfd_adf_read,dset_offsets)\n",
    "# # df_to_csv(vfd_adf_read,fagg,suffix='-vfd-read')\n",
    "\n",
    "\n",
    "df_to_csv(vol_adf_read,fagg,suffix='-vol-read')\n",
    "df_to_csv(vfd_adf_read,fagg,suffix='-vfd-read')\n",
    "\n",
    "df_agg_read_merged = merge_df_agg_read(vol_adf_read,vfd_adf_read,vol_idx2addr_map)\n",
    "print(df_agg_read_merged.columns)\n",
    "\n",
    "df_to_csv(df_agg_read_merged,fagg,suffix='-read-merged')\n",
    "# df_to_parquet(df_agg_read_merged,fagg,suffix='-read-merged')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create lables for AGG_WRITE phase\n",
    "1. create op_type for all 16B, 3KB I/O, and metadata and H5FD_MEM_LHEAP with map \n",
    "``` \n",
    "op_type_map = { # for notes only now\n",
    "    'mem_type' : {\n",
    "        'H5FD_MEM_DRAW' : { 'access_size':{ 16 : 'loc', 'others' : 'data'} },\n",
    "        'H5FD_MEM_LHEAP' : 'lheap','H5FD_MEM_OHDR': 'meta','H5FD_MEM_SUPER': 'meta'}\n",
    "}\n",
    "```\n",
    "2. create data_label for all H5FD_MEM_DRAW that are not 16B (location data)\n",
    "3. map data_label with start_addr-3KB to end_addr-16B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['data_label_vol', 'io_access_idx_vol', 'dset_name', 'access_size_vol',\n",
      "       'logical_addr_vol', 'op_type_vol', 'operation_vol', 'n_elements',\n",
      "       'dimension_cnt', 'dimensions', 'file_intent', 'layout', 'time(us)_vol',\n",
      "       'logical_addr_map', 'data_label_vfd', 'io_access_idx_vfd',\n",
      "       'access_size_vfd', 'logical_addr_vfd', 'next_addr', 'op_type_vfd',\n",
      "       'file_name', 'time(us)_vfd', 'operation_vfd'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# map only write\n",
    "\n",
    "def vfd_op_type_agg_write(df):\n",
    "    op_type_list = []\n",
    "    # tmp_row = pd.DataFrame(columns=list(df.columns))\n",
    "    for index, row in df.iterrows():\n",
    "        if row['mem_type'] == 'H5FD_MEM_DRAW':\n",
    "            if row['access_size'] == 16:\n",
    "                op_type_list.append('loc')\n",
    "            else:\n",
    "                op_type_list.append('data')\n",
    "        elif row['mem_type'] == 'H5FD_MEM_LHEAP':\n",
    "            op_type_list.append('lheap')\n",
    "        else:\n",
    "            op_type_list.append('meta')\n",
    "    \n",
    "    df['op_type'] = op_type_list\n",
    "    return df\n",
    "\n",
    "        \n",
    "def vfd_data_label_agg_write(df, PC_IO_SIZE):\n",
    "    # TODO: needs improvement\n",
    "\n",
    "    data_idx = 0\n",
    "    pc_idx = 0\n",
    "    lheap_idx = 0\n",
    "        \n",
    "    data_label_list = []\n",
    "    loc_map = {}\n",
    "    data_label_map  = {}\n",
    "    \n",
    "    first_chunk_index = 0 # record index at dataframe\n",
    "    prev_address = df[df['op_type'] == 'data'].iloc[0]['start_addr'] # first data address\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        _pat = row['op_type']\n",
    "        if _pat == 'data':\n",
    "            if row['access_size'] == PC_IO_SIZE:\n",
    "                data_label_list.append(f'vfd-data-point_cloud-{pc_idx}')\n",
    "                pc_idx +=1\n",
    "            else:\n",
    "                _str = f'vfd-data-contact_map-{data_idx}'\n",
    "                data_label_list.append(_str)\n",
    "                if first_chunk_index == 0:\n",
    "                    if row['start_addr'] != prev_address:\n",
    "                        first_chunk_index = index\n",
    "                else:\n",
    "                    data_label_map[row['start_addr']] = _str\n",
    "                data_idx +=1\n",
    "                prev_address = row['next_addr']\n",
    "                \n",
    "        elif _pat == 'meta':\n",
    "            data_label_list.append(f'vfd-{_pat}-1')\n",
    "        elif _pat == 'lheap':\n",
    "            data_label_list.append(f'vfd-{_pat}-1-{lheap_idx}')\n",
    "            lheap_idx+=1\n",
    "        else: \n",
    "            if loc_map:\n",
    "                data_label_list.append('0')\n",
    "            else:\n",
    "                data_label_list.append(f'vfd-lheap-1')\n",
    "            loc_map[row['next_addr']] = int(row['io_access_idx'])\n",
    "    \n",
    "    # map some 16B with data_label\n",
    "    for k in data_label_map.keys():\n",
    "        data_label_list[loc_map[k]] = data_label_map[k]\n",
    "    \n",
    "    # find and map the rest 16B with initial chunk\n",
    "    indices = [i for i, x in enumerate(data_label_list) if x == '0']\n",
    "    data_idx = 0\n",
    "    for idx in indices:\n",
    "        data_label_list[idx] = f'vfd-data-contact_map-{data_idx}'\n",
    "        data_idx+=1\n",
    "\n",
    "    df['data_label'] = data_label_list\n",
    "\n",
    "def get_point_cloud_io_size(dfvol):\n",
    "    idx = dfvol.dset_name.str.match('point_cloud').idxmax()\n",
    "    # dims = ast.literal_eval(lastrow['dimensions'])\n",
    "    first_dim = dfvol.iloc[idx]['dimensions'][0] #lastrow['dimensions'][idx]\n",
    "    access_size = dfvol.iloc[idx]['access_size']\n",
    "    io_size = access_size / first_dim\n",
    "    return io_size\n",
    "\n",
    "def hard_code_start_addr_agg(dfvol, dfvfd):\n",
    "    # # hardcode starting address\n",
    "    for dset in ['contact_map', 'point_cloud']:\n",
    "        idx_vfd = dfvfd.data_label.str.match(f'vfd-data-{dset}-0').idxmax()\n",
    "        START_ADDR = dfvfd.iloc[idx_vfd]['start_addr']\n",
    "        \n",
    "        idx_vol = dfvol.data_label.str.match(f'vol-data-{dset}-0').idxmax()\n",
    "        dfvol.loc[idx_vol, ['logical_addr']] = START_ADDR\n",
    "\n",
    "        idx_vol = dfvol.data_label.str.match(f'vol-data-{dset}-1').idxmax()\n",
    "        if idx_vol:\n",
    "            dfvol.loc[idx_vol, ['logical_addr']] = START_ADDR\n",
    "    \n",
    "    return dfvol\n",
    "\n",
    "def merge_df_agg_write(dfvol, dfvfd):\n",
    "    df_agg2vol = dfvol[['data_label', 'io_access_idx','dset_name', 'access_size','logical_addr','op_type', \n",
    "                        'operation', 'n_elements','dimension_cnt','dimensions','file_intent','layout','time(us)']].copy()\n",
    "    df_vol2vfd = dfvfd[['data_label', 'io_access_idx', 'access_size', 'start_addr', 'next_addr','op_type',\n",
    "                        'file_name', 'time(us)','operation']].copy()\n",
    "\n",
    "    # vol map # set 0 for meta\n",
    "    df_agg2vol['logical_addr_map'] = df_agg2vol['logical_addr'].astype(int)\n",
    "    df_agg2vol.loc[df_agg2vol['op_type'].str.match('meta'), 'logical_addr_map'] = 0\n",
    "    \n",
    "    # data map\n",
    "    df_vfd2res_data = df_vol2vfd[df_vol2vfd['op_type'] == 'data'].copy()\n",
    "    df_vfd2res_data['logical_addr_map'] = df_vfd2res_data['start_addr'].astype(int)\n",
    "    # modify point_cloud to use only 1 address for mapping \n",
    "    idx_pc = df_agg2vol.data_label.str.match(f'vol-data-point_cloud-0').idxmax()\n",
    "    PC_START_ADDR = int(df_agg2vol.iloc[idx_pc]['logical_addr'])\n",
    "    df_vfd2res_data.loc[df_vfd2res_data['data_label'].str.contains('point_cloud'), 'logical_addr_map'] = PC_START_ADDR\n",
    "    \n",
    "    # loc maps by data_label \n",
    "    df_vfd2res_loc = df_vol2vfd[df_vol2vfd['op_type'] == 'loc'].copy()\n",
    "    vfd2res_loc_map  = dict(zip(df_vfd2res_data['data_label'], df_vfd2res_data['start_addr']))\n",
    "    df_vfd2res_loc['logical_addr_map'] = df_vfd2res_loc['data_label'].map(vfd2res_loc_map)\n",
    "    \n",
    "    # lheap maps by address\n",
    "    df_vfd2res_lheap = df_vol2vfd[df_vol2vfd['op_type'] == 'lheap'].copy()\n",
    "    vfd2res_lheap_map = dict(zip(df_vfd2res_data['start_addr'], df_vfd2res_data['next_addr']))\n",
    "    df_vfd2res_lheap['logical_addr_map'] = df_vfd2res_lheap['next_addr'].map(vfd2res_lheap_map)\n",
    "    \n",
    "    # meta maps to 0\n",
    "    df_vfd2res_meta = df_vol2vfd[df_vol2vfd['op_type'] == 'meta'].copy()\n",
    "    df_vfd2res_meta['logical_addr_map'] = 0\n",
    "    \n",
    "    df_vfd2res = pd.concat([df_vfd2res_data,df_vfd2res_loc,df_vfd2res_lheap,df_vfd2res_meta], axis=0) #.set_index('io_access_idx')\n",
    "    df_vfd2res = df_vfd2res.rename(columns={'start_addr':'logical_addr'})\n",
    "    df_merged = df_agg2vol.merge(df_vfd2res, how='inner', on=['logical_addr_map'], suffixes=['_vol','_vfd'])\n",
    "\n",
    "    # populate file_name\n",
    "    df_merged['file_name'] = df_merged['file_name'].fillna('aggregate.h5')\n",
    "    # df_merged['access_size_vol'] = df_merged['access_size_vol'].fillna(0)\n",
    "    # df_merged['access_size_vol'] = df_merged['access_size_vol'].replace(np.nan, 0)\n",
    "    \n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "vfd_adf_write = vfd_adf[vfd_adf['cat'] == 'agg-write'].copy().reset_index()\n",
    "vol_adf_write = vol_adf[vol_adf['cat'] == 'agg-write'].copy().reset_index()\n",
    "\n",
    "# check_empty(vol_adf_write)\n",
    "\n",
    "# add data_label for vol\n",
    "vol_adf_write['dset_name'] = vol_adf_write['dset_name'].fillna(method='bfill')\n",
    "vol_adf_write['dset_name'] = vol_adf_write['dset_name'].fillna(method='ffill')\n",
    "add_vol_op_type(vol_adf_write)\n",
    "vol_data_label(vol_adf_write)\n",
    "vol_adf_write.loc[vol_adf_write['op_type'] == 'meta', 'io_access_idx'] = -1\n",
    "\n",
    "vfd_adf_write = vfd_op_type_agg_write(vfd_adf_write)\n",
    "PC_IO_SIZE = get_point_cloud_io_size(vol_adf_write)\n",
    "vfd_data_label_agg_write(vfd_adf_write, PC_IO_SIZE)\n",
    "\n",
    "\n",
    "# get logical address from vfd to vol\n",
    "vol_adf_write = hard_code_start_addr_agg(vol_adf_write,vfd_adf_write)\n",
    "\n",
    "df_to_csv(vol_adf_write,fagg,suffix='-vol-write')\n",
    "df_to_csv(vfd_adf_write,fagg,suffix='-vfd-write')\n",
    "# df_to_parquet(vol_adf_write,fagg,suffix='-vol-write')\n",
    "# df_to_parquet(vfd_adf_write,fagg,suffix='-vfd-write')\n",
    "\n",
    "df_agg_write_merged = merge_df_agg_write(vol_adf_write,vfd_adf_write)\n",
    "print(df_agg_write_merged.columns)\n",
    "\n",
    "df_to_csv(df_agg_write_merged,fagg,suffix='-write-merged')\n",
    "# df_to_parquet(df_agg_write_merged,fagg,suffix='-write-merged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b91b7dd4d8a7b6a913637a2227eb724977af8647a7e6e9f1d2538a8f2b15605"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
