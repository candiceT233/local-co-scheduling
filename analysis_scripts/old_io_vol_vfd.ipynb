{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    mode='r'\n",
    "    read_ops = ['dataset_read', 'blob_get', ] #'H5VLdataset_get'\n",
    "    \n",
    "    write_ops = ['dataset_write', 'blob_put']\n",
    "    # 'file_open' # not used, dset has file info\n",
    "    other_ops = ['object_open', 'dataset_open', 'dataset_create', 'object_close', 'dataset_close' ] \n",
    "\n",
    "    index = 0\n",
    "\n",
    "    records = {}\n",
    "    with open(file_name, mode) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                rec = ast.literal_eval(line)\n",
    "                # print(data)\n",
    "                    \n",
    "                if any(op in rec['func_name'] for op in read_ops):\n",
    "                    records[index] = rec\n",
    "                    records[index]['operation'] = 'read'\n",
    "                    index+=1\n",
    "                elif any(op in rec['func_name'] for op in write_ops):\n",
    "                    records[index] = rec\n",
    "                    records[index]['operation'] = 'write'\n",
    "                    index+=1\n",
    "                elif any(op in rec['func_name'] for op in other_ops):\n",
    "                    records[index] = rec\n",
    "                    op_type = rec['func_name'].split('_')[1]\n",
    "                    records[index]['operation'] = op_type\n",
    "                    index+=1\n",
    "                    \n",
    "            except:\n",
    "                print(\"Erro line:\")\n",
    "                print(line)\n",
    "                break\n",
    "    return records\n",
    "\n",
    "def rec_to_df(records):\n",
    "    df = pd.DataFrame.from_dict(records,orient='index')\n",
    "    df.replace('/mnt/ssd/mtang11/','',regex=True, inplace=True)\n",
    "    df.replace('molecular_dynamics_runs/stage0000/','',regex=True, inplace=True)\n",
    "    \n",
    "    # df['hash_id']= df['hash_id'].astype(str)\n",
    "    # df['hash_id'] = map(lambda x: x.encode('base64','strict'), df['hash_id'])\n",
    "    return df\n",
    "\n",
    "def df_to_csc(df,file_name):\n",
    "    df.hash_id=df.hash_id.astype('category').cat.codes\n",
    "    out_csv=file_name.replace('.log','.csv')\n",
    "    # print(df.columns)\n",
    "    df.to_csv(out_csv)\n",
    "\n",
    "def get_hash_id_list(df):\n",
    "    hash_id_list = list(df.hash_id.unique())\n",
    "    if -1 in hash_id_list:\n",
    "        hash_id_list.remove(-1) # -1 is not valid hash_id\n",
    "    return hash_id_list\n",
    "\n",
    "def hash_id_groups(df,file_name):\n",
    "    # observe hash_id and blob_access_index relations\n",
    "    df['access_idx'] = df.index\n",
    "    df = df.fillna(0)\n",
    "    df['total_read_bytes'] = df['total_read_bytes'].astype(int)\n",
    "    df['total_write_bytes'] = df['total_write_bytes'].astype(int)\n",
    "    df = df[df['hash_id'] != -1] # remove hash_id = -1 --> not hashed    \n",
    "    df = df[df['func_name'].str.contains(\"blob\")] # select only blob operations\n",
    "    \n",
    "    grouped = df.groupby('hash_id', group_keys=True)\n",
    "\n",
    "    out_txt=file_name.replace('.log','.txt')\n",
    "    \n",
    "    with open(out_txt, 'w') as f:\n",
    "        for name, group in grouped:\n",
    "            # tmp_df = grouped.get_group(idx)\n",
    "            # tmp_df= tmp_df.sort_values(by=['blob_access_idx'],ascending=True)\n",
    "            f.write(f'hash_id = {name} \\n')\n",
    "            rec = group.to_dict(orient='records')\n",
    "            for r in rec:\n",
    "                f.write(f'{r}\\n')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro line:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fsim='../tmp_outputs/prov-sim.log'\n",
    "fagg='../tmp_outputs/prov-agg.log'\n",
    "\n",
    "\n",
    "sim_rec = read_file(fsim)\n",
    "agg_rec = read_file(fagg)\n",
    "\n",
    "dfsim = rec_to_df(sim_rec)\n",
    "dfagg = rec_to_df(agg_rec)\n",
    "\n",
    "df_to_csc(dfsim,fsim)\n",
    "df_to_csc(dfagg,fagg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(dfsim['blob_access_idx'].max())\n",
    "print(dfagg['blob_access_idx'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same\n"
     ]
    }
   ],
   "source": [
    "sim_hash_id = get_hash_id_list(dfsim)\n",
    "agg_hash_id = get_hash_id_list(dfagg)\n",
    "\n",
    "sim_hash_id = sim_hash_id.sort()\n",
    "agg_hash_id = agg_hash_id.sort()\n",
    "\n",
    "if sim_hash_id == agg_hash_id:\n",
    "    print(\"Same\")\n",
    "else:\n",
    "    print(f'agg_hash_id = {agg_hash_id}')\n",
    "    print(f'agg_hash_id = {agg_hash_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "# print(dfsim.head(10))\n",
    "fsim='tmp_outputs/prov-sim.log'\n",
    "sim_rec = read_file(fsim)\n",
    "dfsim = rec_to_df(sim_rec)\n",
    "dfsim1 = dfsim[dfsim['func_name'].str.contains(\"H5VLblob_put\")] # select only blob operations\n",
    "\n",
    "# dfsim2 = dfsim1.groupby(['blocklist'])['blocklist'].size()\n",
    "# dfsim_idlist = dfsim1.groupby(['blocklist'])['hash_id'].apply(list)\n",
    "# sim_count_list = []\n",
    "sim_hashid_list = []\n",
    "hashid_list = []\n",
    "sim_blobidx_list = []\n",
    "blobidx_list = []\n",
    "\n",
    "# row = next(dfsim1.iterrows())[1]\n",
    "tmp_logical_addr = dfsim1['logical_addr'].iloc[1]\n",
    "hashid_list.append(dfsim1['hash_id'].iloc[0])\n",
    "blobidx_list.append(dfsim1['blob_idx'].iloc[0])\n",
    "# print(tmp_logical_addr)\n",
    "count = 1\n",
    "\n",
    "for index, row in islice(dfsim1.iterrows(), 1, None):\n",
    "    if row['logical_addr'] == tmp_logical_addr:\n",
    "        blobidx_list.append(row['blob_idx'])\n",
    "        hashid_list.append(row['hash_id'])\n",
    "    else:\n",
    "        tmp_logical_addr = row['logical_addr']        \n",
    "        sim_blobidx_list.append(blobidx_list)\n",
    "        blobidx_list = [row['blob_idx'] ]\n",
    "\n",
    "        # tmp_list = hashid_list.copy()\n",
    "        sim_hashid_list.append(hashid_list)\n",
    "        hashid_list= [row['hash_id']]\n",
    "\n",
    "sim_blobidx_list.append(blobidx_list)\n",
    "sim_hashid_list.append(hashid_list)\n",
    "\n",
    "# print(sim_blobidx_list)\n",
    "print(sum(len(l) for l in sim_blobidx_list))\n",
    "# print(sim_hashid_list)\n",
    "# print(dfsim_idlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "same blobidx_list\n",
      "same hashid_list\n"
     ]
    }
   ],
   "source": [
    "f2='tmp_outputs/prov-vfd-agg.log'\n",
    "mode='r'\n",
    "\n",
    "com_rec = {}\n",
    "index=0\n",
    "\n",
    "agg_blobidx_list = []\n",
    "blobidx_list = []\n",
    "agg_hashid_list = []\n",
    "hashid_list = []\n",
    "count = 0\n",
    "\n",
    "with open(f2, mode) as f:\n",
    "        for line in f:\n",
    "            if \"H5VLblob_get\" in line:\n",
    "                rec = ast.literal_eval(line)\n",
    "                # print(data)\n",
    "                # print(rec['blob_idx'])\n",
    "                hashid_list.append(rec['hash_id'])\n",
    "                blobidx_list.append(rec['blob_idx'])\n",
    "                com_rec[index] = rec\n",
    "                com_rec[index]['operation'] = 'read'\n",
    "                index+=1\n",
    "                count +=1\n",
    "\n",
    "            else:\n",
    "                if count != 0:\n",
    "                    agg_blobidx_list.append(blobidx_list)\n",
    "                    blobidx_list = []\n",
    "                    agg_hashid_list.append(hashid_list)\n",
    "                    hashid_list = []\n",
    "                    count = 0\n",
    "\n",
    "# count_list[0]+=1\n",
    "\n",
    "# print(agg_count_list)\n",
    "print(sum(len(l) for l in agg_blobidx_list))\n",
    "\n",
    "\n",
    "if sim_blobidx_list == agg_blobidx_list:\n",
    "    print(\"same blobidx_list\")\n",
    "else:\n",
    "    print(\"different blobidx_list\")\n",
    "    for index, (first, second) in enumerate(zip(sim_blobidx_list, agg_blobidx_list)):\n",
    "        if first != second:\n",
    "            print(index, first, second)\n",
    "\n",
    "if agg_hashid_list == sim_hashid_list:\n",
    "    print(\"same hashid_list\")\n",
    "else:\n",
    "    print(\"different hashid_list\")\n",
    "    for index, (first, second) in enumerate(zip(sim_hashid_list, agg_hashid_list)):\n",
    "        if first != second:\n",
    "            print(index, first, second)\n",
    "\n",
    "# dfagg1 = rec_to_df(com_rec)\n",
    "# dfsim1 = dfagg1[dfagg1['func_name'].str.contains(\"H5VLblob_get\")] # select only blob operations\n",
    "# dfagg1 = dfagg1.groupby(['blocklist'])['blocklist'].size()\n",
    "# dfsim3 = dfsim1.groupby(['blocklist'])['hash_id'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39482\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "agg_blobidx_list2 = []\n",
    "blobidx_list = []\n",
    "agg_hashid_list2 = []\n",
    "hashid_list = []\n",
    "count = 0\n",
    "with open(f2, mode) as f:\n",
    "        for line in f:\n",
    "            if \"H5VLblob_put\" in line:\n",
    "                rec = ast.literal_eval(line)\n",
    "                # print(data)\n",
    "                # print(rec['blob_idx'])\n",
    "                hashid_list.append(rec['hash_id'])\n",
    "                blobidx_list.append(rec['blob_idx'])\n",
    "                com_rec[index] = rec\n",
    "                com_rec[index]['operation'] = 'read'\n",
    "                index+=1\n",
    "                count +=1\n",
    "\n",
    "            else:\n",
    "                if count != 0:\n",
    "                    agg_blobidx_list2.append(blobidx_list)\n",
    "                    blobidx_list = []\n",
    "                    agg_hashid_list2.append(hashid_list)\n",
    "                    hashid_list = []\n",
    "                    count = 0\n",
    "\n",
    "print(len(agg_hashid_list2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000\n",
      "60116\n",
      "different hashid_list\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_410790/451502465.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdiff_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten_agg_hashs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mflatten_agg_hashs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mflatten_agg_hashs2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mdiff_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten_agg_hashs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten_agg_hashs2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "flatten_agg_hashs = list(chain.from_iterable(agg_hashid_list))\n",
    "flatten_agg_hashs2 = list(chain.from_iterable(agg_hashid_list2))\n",
    "print(len(flatten_agg_hashs))\n",
    "print(len(flatten_agg_hashs2))\n",
    "\n",
    "if flatten_agg_hashs == flatten_agg_hashs2:\n",
    "    print(\"same hashid_list\")\n",
    "else:\n",
    "    print(\"different hashid_list\")\n",
    "\n",
    "diff_list = []\n",
    "for i in range(0, len(flatten_agg_hashs)):\n",
    "    if flatten_agg_hashs[i] != flatten_agg_hashs2[i]:\n",
    "        diff_list.append([i, flatten_agg_hashs[i], flatten_agg_hashs2[i]])\n",
    "\n",
    "print(diff_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daskenv",
   "language": "python",
   "name": "daskenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "166794962df2e049c686d26b1cdbecb85abdf0b3ea2599938e12ddd8b5f67bd0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
