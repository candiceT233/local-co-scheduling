{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    mode='r'\n",
    "    read_ops = ['dataset_read', 'blob_get', ] #'H5VLdataset_get'\n",
    "    \n",
    "    write_ops = ['dataset_write', 'blob_put']\n",
    "    # 'file_open' # not used, dset has file info\n",
    "    other_ops = ['object_open', 'dataset_open', 'dataset_create', 'object_close', 'dataset_close' ] \n",
    "\n",
    "    index = 0\n",
    "\n",
    "    records = {}\n",
    "    with open(file_name, mode) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                rec = ast.literal_eval(line)\n",
    "                # print(data)\n",
    "                    \n",
    "                if any(op in rec['func_name'] for op in read_ops):\n",
    "                    records[index] = rec\n",
    "                    records[index]['operation'] = 'read'\n",
    "                    index+=1\n",
    "                elif any(op in rec['func_name'] for op in write_ops):\n",
    "                    records[index] = rec\n",
    "                    records[index]['operation'] = 'write'\n",
    "                    index+=1\n",
    "                elif any(op in rec['func_name'] for op in other_ops):\n",
    "                    records[index] = rec\n",
    "                    op_type = rec['func_name'].split('_')[1]\n",
    "                    records[index]['operation'] = op_type\n",
    "                    index+=1\n",
    "                    \n",
    "            except:\n",
    "                print(\"Erro line:\")\n",
    "                print(line)\n",
    "                break\n",
    "    return records\n",
    "\n",
    "def rec_to_df(records):\n",
    "    df = pd.DataFrame.from_dict(records,orient='index')\n",
    "    df.replace('/mnt/ssd/mtang11/','',regex=True, inplace=True)\n",
    "    df.replace('molecular_dynamics_runs/stage0000/','',regex=True, inplace=True)\n",
    "    \n",
    "    # df['hash_id']= df['hash_id'].astype(str)\n",
    "    # df['hash_id'] = map(lambda x: x.encode('base64','strict'), df['hash_id'])\n",
    "    return df\n",
    "\n",
    "def df_to_csc(df,file_name):\n",
    "    df.hash_id=df.hash_id.astype('category').cat.codes\n",
    "    out_csv=file_name.replace('.log','.csv')\n",
    "    # print(df.columns)\n",
    "    df.to_csv(out_csv)\n",
    "\n",
    "def get_hash_id_list(df):\n",
    "    hash_id_list = list(df.hash_id.unique())\n",
    "    if -1 in hash_id_list:\n",
    "        hash_id_list.remove(-1) # -1 is not valid hash_id\n",
    "    return hash_id_list\n",
    "\n",
    "def hash_id_groups(df,file_name):\n",
    "    # observe hash_id and blob_access_index relations\n",
    "    df['access_idx'] = df.index\n",
    "    df = df.fillna(0)\n",
    "    df['total_read_bytes'] = df['total_read_bytes'].astype(int)\n",
    "    df = df[df['hash_id'] != -1] # remove hash_id = -1 --> not hashed    \n",
    "    df = df[df['func_name'].str.contains(\"blob\")] # select only blob operations\n",
    "    \n",
    "    grouped = df.groupby('hash_id', group_keys=True)\n",
    "\n",
    "    out_txt=file_name.replace('.log','.txt')\n",
    "    \n",
    "    with open(out_txt, 'w') as f:\n",
    "        for name, group in grouped:\n",
    "            # tmp_df = grouped.get_group(idx)\n",
    "            # tmp_df= tmp_df.sort_values(by=['blob_access_idx'],ascending=True)\n",
    "            f.write(f'hash_id = {name} \\n')\n",
    "            rec = group.to_dict(orient='records')\n",
    "            for r in rec:\n",
    "                f.write(f'{r}\\n')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'hash_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_317883/385025816.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdf_to_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfsim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfsim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdf_to_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfagg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfagg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_317883/1280092837.py\u001b[0m in \u001b[0;36mdf_to_csc\u001b[0;34m(df, file_name)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdf_to_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mout_csv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.log'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# print(df.columns)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/common/mtang11/scripts/databots/my_find_path/daskenv/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'hash_id'"
     ]
    }
   ],
   "source": [
    "fsim='tmp_outputs/prov-sim.log'\n",
    "fagg='tmp_outputs/prov-agg.log'\n",
    "\n",
    "\n",
    "sim_rec = read_file(fsim)\n",
    "agg_rec = read_file(fagg)\n",
    "# print(records[0])\n",
    "# print(records[0]['dimensions'][0])\n",
    "# print(list(blob_records.items())[0])\n",
    "# print(blob_records[list(blob_records.keys())[0]])\n",
    "dfsim = rec_to_df(sim_rec)\n",
    "dfagg = rec_to_df(agg_rec)\n",
    "\n",
    "df_to_csc(dfsim,fsim)\n",
    "df_to_csc(dfagg,fagg)\n",
    "\n",
    "\n",
    "\n",
    "hash_id_groups(dfsim,fsim)\n",
    "hash_id_groups(dfagg,fagg)\n",
    "# print(df.head(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(dfsim['blob_access_idx'].max())\n",
    "print(dfagg['blob_access_idx'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same\n"
     ]
    }
   ],
   "source": [
    "sim_hash_id = get_hash_id_list(dfsim)\n",
    "agg_hash_id = get_hash_id_list(dfagg)\n",
    "\n",
    "sim_hash_id = sim_hash_id.sort()\n",
    "agg_hash_id = agg_hash_id.sort()\n",
    "\n",
    "if sim_hash_id == agg_hash_id:\n",
    "    print(\"Same\")\n",
    "else:\n",
    "    print(f'agg_hash_id = {agg_hash_id}')\n",
    "    print(f'agg_hash_id = {agg_hash_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daskenv",
   "language": "python",
   "name": "daskenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "166794962df2e049c686d26b1cdbecb85abdf0b3ea2599938e12ddd8b5f67bd0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
